{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Efficient coding patterns in R and Python\"\n",
        "format: html\n",
        "engine: jupyter\n",
        "---"
      ],
      "id": "960590fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://huggingface.co/datasets/malcolmbarrett/fire_dept_calls/resolve/main/fd_calls.csv\"\n",
        "file_path = \"fd_calls.csv\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn 1\n",
        "\n",
        "Read in the `fd_calls.csv` file. Create a new variable called `log_delay` that is the log of `Delay`. Subset the data frame to just use rows where `year` is 2015. \n",
        "\n",
        "Then try the reverse order: first subset, then create the new variable.\n",
        "\n",
        "Below are examples in the Pandas and Polars; modify whichever style you prefer.\n",
        "\n",
        "### Pandas "
      ],
      "id": "e72b4a68"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fd_calls = pd.read_csv(\"fd_calls.csv\", low_memory=False)"
      ],
      "id": "2139f70d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars "
      ],
      "id": "b238c4a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fd_calls_pl = pl.read_csv(\"fd_calls.csv\", infer_schema_length=75_000)"
      ],
      "id": "7e7977a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn 2\n",
        "\n",
        "Benchmark the two approaches you wrote in Your Turn 1 using timeit First, write a function for each approach, then call the function in timeit.\n",
        "\n",
        "### Polars"
      ],
      "id": "ac50d92a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stretch Goal: Arrow\n",
        "\n",
        "The Arrow specification allows you to do much less. Not only does it store data in a more efficient format (the file size is often substantially smaller), it reads and manipulates data much faster, in part because it only reads in the data it needs into memory.\n",
        "\n",
        "Run this code to create a set of parquet files for each `year`."
      ],
      "id": "a6dfbf2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fd_calls.to_parquet(\"fd_calls\", partition_cols=[\"year\"])"
      ],
      "id": "88eb3ce1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do a slightly more involved complication than above to demonstrate some of Arrow's strengths. The pyarrow package comes with small data manipulation API. We could also read it in then convert to Pandas or Polars with zero copy time."
      ],
      "id": "5df0a051"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "dataset = ds.dataset(\"fd_calls\", format=\"parquet\", partitioning=\"hive\")\n",
        "\n",
        "# Filter by partition column 'year' and perform the aggregation\n",
        "table = (dataset\n",
        "    .to_table(filter=ds.field('year') == 2015)  # Use partition field for filtering\n",
        "    .group_by(['Neighborhood'])\n",
        "    .aggregate([('Delay', 'mean')])\n",
        ")\n",
        "\n",
        "table = table.append_column('log_delay', pa.compute.ln(table['Delay_mean']))"
      ],
      "id": "ac98ab58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a benchmark that compares the Arrow approach with these two approaches using Pandas and Polars."
      ],
      "id": "a1121801"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def arrow():\n",
        "    # fill in the arrow version \n",
        "    \n",
        "def pandas():\n",
        "    fd_calls = pd.read_csv(\"fd_calls.csv\", low_memory=False)\n",
        "    result = (\n",
        "        fd_calls[fd_calls['year'] == 2015]\n",
        "        .groupby('Neighborhood', as_index=False)\n",
        "        .agg(log_delay=('Delay', lambda x: np.mean(np.log(x))))\n",
        "    )\n",
        "    return result\n",
        "\n",
        "def polars():\n",
        "    fd_calls_pl = pl.read_csv(\"fd_calls.csv\", infer_schema_length=75_000)\n",
        "    result = (\n",
        "        fd_calls_pl.filter(pl.col('year') == 2015)\n",
        "        .group_by('Neighborhood')\n",
        "        .agg(pl.col('Delay').log().mean().alias('log_delay'))\n",
        "    )\n",
        "    return result\n",
        "\n",
        "\n",
        "%timeit pandas()\n",
        "\n",
        "%timeit polars()\n",
        "\n",
        "%timeit arrow()"
      ],
      "id": "c2c38fcf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn 3: Challenge!\n",
        "\n",
        "Below, we create three data frames: `population`, `age_effects`, and `condition_effects`. `population` is a simulated group of people. For each person, we want to take their age and condition and calculate a total cost."
      ],
      "id": "aeaaafee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(123)\n",
        "n = 30_000\n",
        "ages = np.random.choice(np.arange(20, 81), size=n, replace=True)\n",
        "conditions = np.random.choice(['Healthy', 'Diabetes', 'Heart Disease'], size=n, replace=True, p=[0.6, 0.3, 0.1])\n",
        "\n",
        "population = pd.DataFrame({\n",
        "    'id': np.arange(1, n + 1),\n",
        "    'age': ages,\n",
        "    'condition': conditions\n",
        "})\n",
        "\n",
        "age_costs = pd.DataFrame({\n",
        "    'age': np.arange(20, 81),\n",
        "    'cost': np.linspace(200, 2000, num=61)\n",
        "})\n",
        "\n",
        "condition_costs = pd.DataFrame({\n",
        "    'condition': ['Healthy', 'Diabetes', 'Heart Disease'],\n",
        "    'cost': [100, 500, 1000]\n",
        "})\n",
        "\n",
        "noise = np.random.normal(size=n)"
      ],
      "id": "493360c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One way to do this is a for loop:"
      ],
      "id": "aaba83c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "population['cost_for_loop'] = np.nan\n",
        "\n",
        "for i in range(len(population)):\n",
        "    age_cost = age_costs.loc[age_costs['age'] == population.loc[i, 'age'], 'cost'].values[0]\n",
        "    condition_cost = condition_costs.loc[condition_costs['condition'] == population.loc[i, 'condition'], 'cost'].values[0]\n",
        "    population.at[i, 'cost_for_loop'] = age_cost + condition_cost + noise[i]"
      ],
      "id": "6f41ee41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But if you run it, you'll see it takes a little bit to run.\n",
        "\n",
        "For this exercise, vectorize this for loop to make it more efficient. Benchmark the two approaches and compare.\n",
        "\n",
        "Here are a couple of clues. First, note that you can sample vectors in Python to a lesser or greater value than the length of the array For instance, to create a array of length 100 from the `age_effects['cost']` array, which has a length of 61, we just subset with the indices for 100 samples."
      ],
      "id": "1e7ee5b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "idx = np.random.choice(\n",
        "    len(age_costs),  # 61 rows\n",
        "    size=100,  # but sample 100 of them\n",
        "    replace=True  # with replacement\n",
        ")\n",
        "\n",
        "len(age_costs['cost'].values[idx])"
      ],
      "id": "86282cce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second clue is that you can use `pd.Index().get_indexer()` to match an array against another. This returns the indices in the second array that match the values of the first array For example, `pd.Index(list(\"abcde\")).get_indexer([\"b\"])` returns 1 because that's where \"b\" is in the alphabet (0-indexed in Python)."
      ],
      "id": "d21c34bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "letters = list('abcdefghijklmnopqrstuvwxyz')\n",
        "pd.Index(letters).get_indexer(['f', 'a', 'a', 'a', 's', 't'])"
      ],
      "id": "c0784c9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# vectorize the above for loop\n"
      ],
      "id": "248a3b85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# benchmark the two approaches"
      ],
      "id": "ec251af6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn 4: Challenge!\n",
        "\n",
        "`profile.py` contains a script that defines the function `sum_squared_diffs()` as well as a matrix called `mat`. Source the code in that file, then profile the function. \n",
        "\n",
        "In the terminal, run: \n",
        "\n",
        "```bash\n",
        "scalene profile.py\n",
        "```\n",
        "\n",
        "Once you've identified the bottleneck, try to improve the speed of the code."
      ],
      "id": "9e1653aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn 5\n",
        "\n",
        "Python comes with many approaches to parallel processing, but we'll use concurrent.futures. First, run this code to set up. This will use 2 fewer than the number of cores on your computer. "
      ],
      "id": "585a4b59"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from os import cpu_count\n",
        "\n",
        "n_cores = cpu_count() - 2\n",
        "times = 10_000"
      ],
      "id": "9ba03a1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code imports a simple bootstrap procedure to calculate the median of `x`. `bootstrap.median()` bootstraps `sim_data` and returns the estimate for that resample. See `bootstrap.py` for the code."
      ],
      "id": "68b26b64"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import bootstrap\n",
        "bootstrap.median()"
      ],
      "id": "b7550e93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert the following code to use `ProcessPoolExecutor`:"
      ],
      "id": "a970d185"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "boot_medians = [bootstrap.median(_) for _ in range(times)]\n",
        "boot_medians = np.array(boot_medians)\n",
        "np.quantile(boot_medians, [0.025, 0.5, 0.975])"
      ],
      "id": "b22bde73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "\n",
        "# Take aways\n",
        "\n",
        "* The fastest way to speed up your code is to do nothing. Try to do less! A simple way is to reduce the amount of data you're working with before making calculations\n",
        "* Benchmarking is a practical way to investigate and experiment with code to understand how long different strategies take\n",
        "* Vectorization is common in the design of many Python statistical tools, particularly numpy, and it's usually faster than trying to vectorize code yourself with for loops\n",
        "* Profiling can help you understand where bottlenecks are in your code so you can be more efficient in optimizing your code\n",
        "* Python has many tools for parallelization; use them when you have many repeated, independent actions. It doesn't speed up the code itself and comes with some over head, but it can be a substantial speed up in programmer time by doing many calculations simultaneously. \n"
      ],
      "id": "883f109e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/malcolmbarrett/core/active/reference/nbqa-test/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}